{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 16:19:03.394048: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW \n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نگاه این روانی تیمارستانی کنید ب بی تی اس میگه...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>این یکی عربه میتونه سیرش کنه خخخخ منظورم رو که...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>دولتی که فسادو رانت خواری تمامش رافراگرفته ازو...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  status\n",
       "0  نگاه این روانی تیمارستانی کنید ب بی تی اس میگه...       0\n",
       "1  این یکی عربه میتونه سیرش کنه خخخخ منظورم رو که...       0\n",
       "2  دولتی که فسادو رانت خواری تمامش رافراگرفته ازو...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"ready_data.xlsx\", index_col=0)\n",
    "labels = [\"rejected\", \"published\"]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id: {'rejected': 0, 'published': 1}\n",
      "id2label: {0: 'rejected', 1: 'published'}\n"
     ]
    }
   ],
   "source": [
    "# Labels\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "print(f'label2id: {label2id}')\n",
    "print(f'id2label: {id2label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Valid Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15329, 2)\n",
      "(1704, 2)\n",
      "(1893, 2)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size=0.1, random_state=1, stratify=df['status'])\n",
    "train, valid = train_test_split(train, test_size=0.1, random_state=1, stratify=train['status'])\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "x_train, y_train = train['comment'].values.tolist(), train['status'].values.tolist()\n",
    "x_valid, y_valid = valid['comment'].values.tolist(), valid['status'].values.tolist()\n",
    "x_test, y_test = test['comment'].values.tolist(), test['status'].values.tolist()\n",
    "\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device: {device}')\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general config\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "TEST_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 3\n",
    "EEVERY_EPOCH = 1000\n",
    "LEARNING_RATE = 2e-5\n",
    "CLIP = 0.0\n",
    "\n",
    "MODEL_NAME_OR_PATH = \"/home/reza/Desktop/aasaam/comment-classification/comment_classification/R&D/model/bert-fa-zwnj-base/\"\n",
    "OUTPUT_PATH = './model/pytorch-output/output.bin'\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"rejected\",\n",
      "    \"1\": \"published\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"published\": 1,\n",
      "    \"rejected\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 42000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "config = BertConfig.from_pretrained(\n",
    "    MODEL_NAME_OR_PATH, **{\n",
    "        'label2id': label2id,\n",
    "        'id2label': id2label,\n",
    "    })\n",
    "\n",
    "print(config.to_json_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: \n",
      "م …یکی یکی تو رو ….. سران رژیم …ظلم پایدار نیست\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(train))\n",
    "sample_comment = train.iloc[idx]['comment']\n",
    "sample_label = train.iloc[idx]['status']\n",
    "\n",
    "print(f'Sample: \\n{sample_comment}\\n{sample_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Comment: م …یکی یکی تو رو ….. سران رژیم …ظلم پایدار نیست\n",
      "   Tokens: م … یکی یکی تو رو … . . سران رژیم … ظلم پایدار نیست\n",
      "Token IDs: [620, 1019, 2157, 2157, 1952, 1957, 1019, 121, 121, 4728, 4627, 1019, 14941, 6124, 2231]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample_comment)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f'  Comment: {sample_comment}')\n",
    "print(f'   Tokens: {tokenizer.convert_tokens_to_string(tokens)}')\n",
    "print(f'Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "\n",
      "input_ids:\n",
      "tensor([[    2,   620,  1019,  2157,  2157,  1952,  1957,  1019,   121,   121,\n",
      "          4728,  4627,  1019, 14941,  6124,  2231,     3,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "token_type_ids:\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "    sample_comment,\n",
    "    max_length=32,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt',  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(f'Keys: {encoding.keys()}\\n')\n",
    "for k in encoding.keys():\n",
    "    print(f'{k}:\\n{encoding[k]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Create a PyTorch dataset for Taaghche. \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, comments, targets=None, label_list=None, max_len=128):\n",
    "        self.comments = comments\n",
    "        self.targets = targets\n",
    "        self.has_target = isinstance(targets, list) or isinstance(targets, np.ndarray)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        \n",
    "        self.label_map = {label: i for i, label in enumerate(label_list)} if isinstance(label_list, list) else {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        comment = str(self.comments[item])\n",
    "\n",
    "        if self.has_target:\n",
    "            target = self.label_map.get(str(self.targets[item]), str(self.targets[item]))\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            comment,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')\n",
    "        \n",
    "        inputs = {\n",
    "            'comment': comment,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "        }\n",
    "\n",
    "        if self.has_target:\n",
    "            inputs['targets'] = torch.tensor(int(target), dtype=torch.long)\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "\n",
    "def create_data_loader(x, y, tokenizer, max_len, batch_size, label_list):\n",
    "    dataset = CommentDataset(\n",
    "        comments=x,\n",
    "        targets=y,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len,\n",
    "        label_list=label_list)\n",
    "    \n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"rejected\", \"published\"]\n",
    "train_data_loader = create_data_loader(train['comment'].to_numpy(), train['status'].to_numpy(), tokenizer, MAX_LEN, TRAIN_BATCH_SIZE, label_list)\n",
    "valid_data_loader = create_data_loader(valid['comment'].to_numpy(), valid['status'].to_numpy(), tokenizer, MAX_LEN, VALID_BATCH_SIZE, label_list)\n",
    "test_data_loader = create_data_loader(test['comment'].to_numpy(), None, tokenizer, MAX_LEN, TEST_BATCH_SIZE, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['comment', 'input_ids', 'attention_mask', 'token_type_ids', 'targets'])\n",
      "['غلط کردن اصلا حقوق\\u200cها اضافه نشده و مثل ماه قبل پرداخت کردن چرا آنقدر دروغ میگید خدا لعنتشون کنه', 'این قدر سانسور نکن نظرات مثبت سلام فرمانده رو منتشر کن؛ سانسور چی … …', 'این در حالی است که طرف مقابل عدم پایبندی کامل داشته است، یعنی آمریکا نقض کامل و اروپا هم نقض محدود. اروپا هم در این نقض در کنار آمریکا بوده است.', 'یکی به فاحشه\\u200cهای رسمی وکد دار اضافه شد. مردم اینارو شناختن']\n",
      "torch.Size([4, 128])\n",
      "tensor([    2,  8010,  2263,  4383,  3549,  2001,  3032,  3194,   623,  2773,\n",
      "         2208,  2685,  2698,  2263,  2746,   595, 38275,  2222,  7169, 39830,\n",
      "         4340, 24827,  3797,  4461,     3,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "torch.Size([4, 128])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([4, 128])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([4])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "sample_data = next(iter(train_data_loader))\n",
    "\n",
    "print(sample_data.keys())\n",
    "\n",
    "print(sample_data['comment'])\n",
    "print(sample_data['input_ids'].shape)\n",
    "print(sample_data['input_ids'][0, :])\n",
    "print(sample_data['attention_mask'].shape)\n",
    "print(sample_data['attention_mask'][0, :])\n",
    "print(sample_data['token_type_ids'].shape)\n",
    "print(sample_data['token_type_ids'][0, :])\n",
    "print(sample_data['targets'].shape)\n",
    "print(sample_data['targets'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['comment', 'input_ids', 'attention_mask', 'token_type_ids'])\n"
     ]
    }
   ],
   "source": [
    "sample_test = next(iter(test_data_loader))\n",
    "print(sample_test.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(SentimentModel, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME_OR_PATH, return_dict=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 25 16:19:08 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   50C    P8     9W /  N/A |     13MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1216      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A      2567      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "pt_model = None\n",
    "\n",
    "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/reza/Desktop/aasaam/comment-classification/comment_classification/R&D/model/bert-fa-zwnj-base/ were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/reza/Desktop/aasaam/comment-classification/comment_classification/R&D/model/bert-fa-zwnj-base/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_model <class '__main__.SentimentModel'>\n"
     ]
    }
   ],
   "source": [
    "pt_model = SentimentModel(config=config)\n",
    "pt_model = pt_model.to(device)\n",
    "\n",
    "print('pt_model', type(pt_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0919, -0.4562],\n",
      "        [-0.0756, -0.0610],\n",
      "        [-0.5345,  0.0469],\n",
      "        [-0.0238, -0.3289]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.0919, -0.4562],\n",
      "        [-0.0756, -0.0610],\n",
      "        [-0.5345,  0.0469],\n",
      "        [-0.0238, -0.3289]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([0, 1, 1, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# sample data output\n",
    "\n",
    "sample_data_comment = sample_data['comment']\n",
    "sample_data_input_ids = sample_data['input_ids']\n",
    "sample_data_attention_mask = sample_data['attention_mask']\n",
    "sample_data_token_type_ids = sample_data['token_type_ids']\n",
    "sample_data_targets = sample_data['targets']\n",
    "\n",
    "# available for using in GPU\n",
    "sample_data_input_ids = sample_data_input_ids.to(device)\n",
    "sample_data_attention_mask = sample_data_attention_mask.to(device)\n",
    "sample_data_token_type_ids = sample_data_token_type_ids.to(device)\n",
    "sample_data_targets = sample_data_targets.to(device)\n",
    "\n",
    "\n",
    "# outputs = F.softmax(\n",
    "#     pt_model(sample_data_input_ids, sample_data_attention_mask, sample_data_token_type_ids), \n",
    "#     dim=1)\n",
    "\n",
    "outputs = pt_model(sample_data_input_ids, sample_data_attention_mask, sample_data_token_type_ids)\n",
    "print(outputs)\n",
    "_, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "print(outputs[:5, :])\n",
    "print(preds[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).mean()\n",
    "\n",
    "def acc_and_f1(y_true, y_pred, average='weighted'):\n",
    "    acc = simple_accuracy(y_true, y_pred)\n",
    "    f1 = f1_score(y_true=y_true, y_pred=y_pred, average=average)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "def y_loss(y_true, y_pred, losses):\n",
    "    y_true = torch.stack(y_true).cpu().detach().numpy()\n",
    "    y_pred = torch.stack(y_pred).cpu().detach().numpy()\n",
    "    y = [y_true, y_pred]\n",
    "    loss = np.mean(losses)\n",
    "\n",
    "    return y, loss\n",
    "\n",
    "\n",
    "def eval_op(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dl in tqdm(data_loader, total=len(data_loader), desc=\"Evaluation... \"):\n",
    "            \n",
    "            input_ids = dl['input_ids']\n",
    "            attention_mask = dl['attention_mask']\n",
    "            token_type_ids = dl['token_type_ids']\n",
    "            targets = dl['targets']\n",
    "\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "            \n",
    "            # convert output probabilities to predicted class\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # accumulate all the losses\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(targets)\n",
    "    \n",
    "    eval_y, eval_loss = y_loss(y_true, y_pred, losses)\n",
    "    return eval_y, eval_loss\n",
    "\n",
    "\n",
    "def train_op(model, \n",
    "            data_loader, \n",
    "            loss_fn, \n",
    "            optimizer, \n",
    "            scheduler, \n",
    "            step=0, \n",
    "            print_every_step=100, \n",
    "            eval=False,\n",
    "            eval_cb=None,\n",
    "            eval_loss_min=np.Inf,\n",
    "            eval_data_loader=None, \n",
    "            clip=0.0):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for dl in tqdm(data_loader, total=len(data_loader), desc=\"Training... \"):\n",
    "        step += 1\n",
    "\n",
    "        input_ids = dl['input_ids']\n",
    "        attention_mask = dl['attention_mask']\n",
    "        token_type_ids = dl['token_type_ids']\n",
    "        targets = dl['targets']\n",
    "\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # accumulate all the losses\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        if clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
    "\n",
    "        # perform optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        # perform scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(targets)\n",
    "\n",
    "        if eval:\n",
    "            train_y, train_loss = y_loss(y_true, y_pred, losses)\n",
    "            train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')\n",
    "\n",
    "            if step % print_every_step == 0:\n",
    "                eval_y, eval_loss = eval_op(model, eval_data_loader, loss_fn)\n",
    "                eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')\n",
    "\n",
    "                if hasattr(eval_cb, '__call__'):\n",
    "                    eval_loss_min = eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min)\n",
    "\n",
    "    train_y, train_loss = y_loss(y_true, y_pred, losses)\n",
    "\n",
    "    return train_y, train_loss, step, eval_loss_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs... :   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluation... : 100%|██████████| 426/426 [00:23<00:00, 18.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3...Step: 1000...Train Loss: 0.617827...Train Acc: 0.651...Valid Loss: 0.608805...Valid Acc: 0.651...\n",
      "Validation loss decreased (inf --> 0.608805).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluation... : 100%|██████████| 426/426 [00:23<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3...Step: 2000...Train Loss: 0.595483...Train Acc: 0.683...Valid Loss: 0.571611...Valid Acc: 0.710...\n",
      "Validation loss decreased (0.608805 --> 0.571611).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluation... : 100%|██████████| 426/426 [00:23<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3...Step: 3000...Train Loss: 0.584254...Train Acc: 0.692...Valid Loss: 0.542991...Valid Acc: 0.728...\n",
      "Validation loss decreased (0.571611 --> 0.542991).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training... :  94%|█████████▍| 3615/3833 [18:24<01:06,  3.27it/s]\n",
      "Epochs... :   0%|          | 0/3 [18:24<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/reza/Desktop/aasaam/comment-classification/comment_classification/R&D/train-pytorch.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=40'>41</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m eval_cb\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpochs... \u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=44'>45</a>\u001b[0m     train_y, train_loss, step, eval_loss_min \u001b[39m=\u001b[39m train_op(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=45'>46</a>\u001b[0m         model\u001b[39m=\u001b[39;49mpt_model, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=46'>47</a>\u001b[0m         data_loader\u001b[39m=\u001b[39;49mtrain_data_loader, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=47'>48</a>\u001b[0m         loss_fn\u001b[39m=\u001b[39;49mloss_fn, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=48'>49</a>\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=49'>50</a>\u001b[0m         scheduler\u001b[39m=\u001b[39;49mscheduler, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=50'>51</a>\u001b[0m         step\u001b[39m=\u001b[39;49mstep, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=51'>52</a>\u001b[0m         print_every_step\u001b[39m=\u001b[39;49mEEVERY_EPOCH, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=52'>53</a>\u001b[0m         \u001b[39meval\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=53'>54</a>\u001b[0m         eval_cb\u001b[39m=\u001b[39;49meval_callback(epoch, EPOCHS, OUTPUT_PATH),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=54'>55</a>\u001b[0m         eval_loss_min\u001b[39m=\u001b[39;49meval_loss_min,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=55'>56</a>\u001b[0m         eval_data_loader\u001b[39m=\u001b[39;49mvalid_data_loader, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=56'>57</a>\u001b[0m         clip\u001b[39m=\u001b[39;49mCLIP)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=58'>59</a>\u001b[0m     train_score \u001b[39m=\u001b[39m acc_and_f1(train_y[\u001b[39m0\u001b[39m], train_y[\u001b[39m1\u001b[39m], average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=60'>61</a>\u001b[0m     eval_y, eval_loss \u001b[39m=\u001b[39m eval_op(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=61'>62</a>\u001b[0m         model\u001b[39m=\u001b[39mpt_model, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=62'>63</a>\u001b[0m         data_loader\u001b[39m=\u001b[39mvalid_data_loader, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=63'>64</a>\u001b[0m         loss_fn\u001b[39m=\u001b[39mloss_fn)\n",
      "\u001b[1;32m/home/reza/Desktop/aasaam/comment-classification/comment_classification/R&D/train-pytorch.ipynb Cell 30\u001b[0m in \u001b[0;36mtrain_op\u001b[0;34m(model, data_loader, loss_fn, optimizer, scheduler, step, print_every_step, eval, eval_cb, eval_loss_min, eval_data_loader, clip)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=130'>131</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39meval\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=131'>132</a>\u001b[0m     train_y, train_loss \u001b[39m=\u001b[39m y_loss(y_true, y_pred, losses)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=132'>133</a>\u001b[0m     train_score \u001b[39m=\u001b[39m acc_and_f1(train_y[\u001b[39m0\u001b[39;49m], train_y[\u001b[39m1\u001b[39;49m], average\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mweighted\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=134'>135</a>\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m print_every_step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=135'>136</a>\u001b[0m         eval_y, eval_loss \u001b[39m=\u001b[39m eval_op(model, eval_data_loader, loss_fn)\n",
      "\u001b[1;32m/home/reza/Desktop/aasaam/comment-classification/comment_classification/R&D/train-pytorch.ipynb Cell 30\u001b[0m in \u001b[0;36macc_and_f1\u001b[0;34m(y_true, y_pred, average)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39macc_and_f1\u001b[39m(y_true, y_pred, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=4'>5</a>\u001b[0m     acc \u001b[39m=\u001b[39m simple_accuracy(y_true, y_pred)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=5'>6</a>\u001b[0m     f1 \u001b[39m=\u001b[39m f1_score(y_true\u001b[39m=\u001b[39;49my_true, y_pred\u001b[39m=\u001b[39;49my_pred, average\u001b[39m=\u001b[39;49maverage)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=7'>8</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39macc\u001b[39m\u001b[39m\"\u001b[39m: acc,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=8'>9</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m: f1,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reza/Desktop/aasaam/comment-classification/comment_classification/R%26D/train-pytorch.ipynb#ch0000048?line=9'>10</a>\u001b[0m     }\n",
      "File \u001b[0;32m~/Desktop/aasaam/comment-classification/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1132\u001b[0m, in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf1_score\u001b[39m(\n\u001b[1;32m    998\u001b[0m     y_true,\n\u001b[1;32m    999\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1006\u001b[0m ):\n\u001b[1;32m   1007\u001b[0m     \u001b[39m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \n\u001b[1;32m   1009\u001b[0m \u001b[39m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[39m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1132\u001b[0m     \u001b[39mreturn\u001b[39;00m fbeta_score(\n\u001b[1;32m   1133\u001b[0m         y_true,\n\u001b[1;32m   1134\u001b[0m         y_pred,\n\u001b[1;32m   1135\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1136\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1137\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1138\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1139\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1140\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1141\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/aasaam/comment-classification/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1270\u001b[0m, in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfbeta_score\u001b[39m(\n\u001b[1;32m   1145\u001b[0m     y_true,\n\u001b[1;32m   1146\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1154\u001b[0m ):\n\u001b[1;32m   1155\u001b[0m     \u001b[39m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \n\u001b[1;32m   1157\u001b[0m \u001b[39m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[39m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1270\u001b[0m     _, _, f, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m   1271\u001b[0m         y_true,\n\u001b[1;32m   1272\u001b[0m         y_pred,\n\u001b[1;32m   1273\u001b[0m         beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[1;32m   1274\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1275\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1276\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1277\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mf-score\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[1;32m   1278\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1279\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1280\u001b[0m     )\n\u001b[1;32m   1281\u001b[0m     \u001b[39mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m~/Desktop/aasaam/comment-classification/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1556\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1555\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1556\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[1;32m   1558\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/aasaam/comment-classification/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1357\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m average_options \u001b[39mand\u001b[39;00m average \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1355\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39maverage has to be one of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1357\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m   1358\u001b[0m \u001b[39m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[39m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m present_labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/Desktop/aasaam/comment-classification/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:111\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         unique_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49munion1d(y_true, y_pred)\n\u001b[1;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         \u001b[39m# We expect y_true and y_pred to be of the same data type.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m         \u001b[39m# If `y_true` was provided to the classifier as strings,\u001b[39;00m\n\u001b[1;32m    115\u001b[0m         \u001b[39m# `y_pred` given by the classifier will also be encoded with\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[39m# strings. So we raise a meaningful error\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLabels in y_true and y_pred should be of the same type. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot y_true=\u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39munique(y_true)\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe true labels.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munion1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/aasaam/comment-classification/.venv/lib/python3.8/site-packages/numpy/lib/arraysetops.py:781\u001b[0m, in \u001b[0;36munion1d\u001b[0;34m(ar1, ar2)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_union1d_dispatcher)\n\u001b[1;32m    748\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munion1d\u001b[39m(ar1, ar2):\n\u001b[1;32m    749\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[39m    Find the union of two arrays.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[39m    array([1, 2, 3, 4, 6])\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 781\u001b[0m     \u001b[39mreturn\u001b[39;00m unique(np\u001b[39m.\u001b[39;49mconcatenate((ar1, ar2), axis\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m))\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/aasaam/comment-classification/.venv/lib/python3.8/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[1;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[1;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/aasaam/comment-classification/.venv/lib/python3.8/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[39m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     ar\u001b[39m.\u001b[39;49msort()\n\u001b[1;32m    337\u001b[0m     aux \u001b[39m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(aux\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mbool_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(pt_model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "step = 0\n",
    "eval_loss_min = np.Inf\n",
    "history = collections.defaultdict(list)\n",
    "\n",
    "\n",
    "def eval_callback(epoch, epochs, output_path):\n",
    "    def eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min):\n",
    "        statement = ''\n",
    "        statement += 'Epoch: {}/{}...'.format(epoch, epochs)\n",
    "        statement += 'Step: {}...'.format(step)\n",
    "        \n",
    "        statement += 'Train Loss: {:.6f}...'.format(train_loss)\n",
    "        statement += 'Train Acc: {:.3f}...'.format(train_score['acc'])\n",
    "\n",
    "        statement += 'Valid Loss: {:.6f}...'.format(eval_loss)\n",
    "        statement += 'Valid Acc: {:.3f}...'.format(eval_score['acc'])\n",
    "\n",
    "        print(statement)\n",
    "\n",
    "        if eval_loss <= eval_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                eval_loss_min,\n",
    "                eval_loss))\n",
    "            \n",
    "            torch.save(model.state_dict(), output_path)\n",
    "            eval_loss_min = eval_loss\n",
    "        \n",
    "        return eval_loss_min\n",
    "\n",
    "\n",
    "    return eval_cb\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(1, EPOCHS + 1), desc=\"Epochs... \"):\n",
    "    train_y, train_loss, step, eval_loss_min = train_op(\n",
    "        model=pt_model, \n",
    "        data_loader=train_data_loader, \n",
    "        loss_fn=loss_fn, \n",
    "        optimizer=optimizer, \n",
    "        scheduler=scheduler, \n",
    "        step=step, \n",
    "        print_every_step=EEVERY_EPOCH, \n",
    "        eval=True,\n",
    "        eval_cb=eval_callback(epoch, EPOCHS, OUTPUT_PATH),\n",
    "        eval_loss_min=eval_loss_min,\n",
    "        eval_data_loader=valid_data_loader, \n",
    "        clip=CLIP)\n",
    "    \n",
    "    train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')\n",
    "    \n",
    "    eval_y, eval_loss = eval_op(\n",
    "        model=pt_model, \n",
    "        data_loader=valid_data_loader, \n",
    "        loss_fn=loss_fn)\n",
    "    \n",
    "    eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')\n",
    "    \n",
    "    history['train_acc'].append(train_score['acc'])\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(eval_score['acc'])\n",
    "    history['val_loss'].append(eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, comments, tokenizer, max_len=128, batch_size=32):\n",
    "    data_loader = create_data_loader(comments, None, tokenizer, max_len, batch_size, None)\n",
    "    \n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for dl in tqdm(data_loader, position=0):\n",
    "            input_ids = dl['input_ids']\n",
    "            attention_mask = dl['attention_mask']\n",
    "            token_type_ids = dl['token_type_ids']\n",
    "\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.to(device)\n",
    "            \n",
    "            # compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "            \n",
    "            # convert output probabilities to predicted class\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(F.softmax(outputs, dim=1))\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu().detach().numpy()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu().detach().numpy()\n",
    "\n",
    "    return predictions, prediction_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = test['comment'].to_numpy()\n",
    "preds, probs = predict(pt_model, test_comments, tokenizer, max_len=128)\n",
    "\n",
    "print(preds.shape, probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, y_pred = [label_list.index(label) for label in test['status'].values], preds\n",
    "\n",
    "print(f'F1: {f1_score(y_test, y_pred, average=\"weighted\")}')\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, target_names=label_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('comment-classification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08b54b589637f87373e8394536c11cb44a56dcb99faa188bb705919399e0b2bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
